{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dba560",
   "metadata": {},
   "source": [
    "# Format HSC Training Data\n",
    "This notebook demonstrates how to format the data produced from training_data.ipynb into a json file following \n",
    "the COCO format necessary for instance segmentation models in detectron2. This step can take a while, but saves a lot of time once the file is formatted and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c2a2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, \"/home/g4merz/deblend/astrodet/\")\n",
    "\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import SimpleTrainer\n",
    "from detectron2.engine import HookBase\n",
    "from typing import Dict, List, Optional\n",
    "import detectron2.solver as solver\n",
    "import detectron2.modeling as modeler\n",
    "import detectron2.data as data\n",
    "import detectron2.data.transforms as T\n",
    "import detectron2.checkpoint as checkpointer\n",
    "from detectron2.data import detection_utils as utils\n",
    "import weakref\n",
    "import copy\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "from astrodet import astrodet as toolkit\n",
    "from PIL import Image, ImageEnhance\n",
    "from astropy.visualization import make_lupton_rgb\n",
    "from astrodet.detectron import plot_stretch_Q\n",
    "from detectron2.utils.file_io import PathManager\n",
    "from iopath.common.file_io import file_lock\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2fad3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "1.20.3\n",
      "4.5.3\n"
     ]
    }
   ],
   "source": [
    "# Print the versions to test the imports and so we know what works\n",
    "print(detectron2.__version__)\n",
    "print(np.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b2c4ad-3932-4a06-9166-c60fe4b53e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prettify the plotting\n",
    "from astrodet.astrodet import set_mpl_style\n",
    "\n",
    "set_mpl_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae37cf-454d-412e-bfb5-af0cef27498a",
   "metadata": {},
   "source": [
    "### First, get some HSC data from training_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055b5ccc-fbf8-4960-bb7c-7d261fc20474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dirpath = \"/home/shared/hsc/HSC/HSC_DR3/data/\"  # Path to dataset\n",
    "output_dir = \"/home/shared/hsc/HSC/HSC_DR3/models/\"\n",
    "\n",
    "dataset_names = [\"train\", \"test\", \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf90cf2-ca2b-4a42-bca7-9c4529aebfdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6958fcd1-5ce0-43f0-9eee-f2d877148036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block is for debug purposes, set to -1 to include every sample\n",
    "sampleNumbers = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4286c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from detectron2.structures import BoxMode\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "\n",
    "# Yufeng Jun19 add test here\n",
    "dataset_names = [\"train\", \"test\", \"val\"]  # train\n",
    "filenames_dict_list = []  # List holding filenames_dict for each dataset\n",
    "\n",
    "for i, d in enumerate(dataset_names):\n",
    "    data_path = os.path.join(dirpath, d)\n",
    "\n",
    "    # Get dataset dict info\n",
    "    filenames_dict = {}\n",
    "    filenames_dict[\"filters\"] = [\"g\", \"r\", \"i\"]\n",
    "\n",
    "    # Get each unqiue tract-patch in the data directory\n",
    "    # file = full path name\n",
    "    files = glob.glob(os.path.join(data_path, \"*_scarlet_segmask.fits\"))\n",
    "    if sampleNumbers != -1:\n",
    "        files = files[:sampleNumbers]\n",
    "    # s = sample name\n",
    "    s = [os.path.basename(f).split(\"_scarlet_segmask.fits\")[0] for f in files]\n",
    "    # print(f'Tract-patch List: {s}')\n",
    "    for f in filenames_dict[\"filters\"]:\n",
    "        filenames_dict[f] = {}\n",
    "        # List of image files in the dataset\n",
    "        # Yufeng dec/21  [Errno 2] No such file or directory: '/home/shared/hsc/test/G-I-8525-4,5-c5_scarlet_img'\n",
    "        # filenames_dict[f]['img'] = [os.path.join(data_path, f'{f.upper()}-{tract_patch}_scarlet_img.fits') for tract_patch in s]\n",
    "        # Yufeng jan 18 f.upper() indicates filter, tract_patch[1:] removes the default I band in the front\n",
    "        filenames_dict[f][\"img\"] = [\n",
    "            os.path.join(data_path, f.upper() + f\"{tract_patch[1:]}_scarlet_img.fits\") for tract_patch in s\n",
    "        ]\n",
    "        # List of mask files in the dataset\n",
    "        # Yufeng jan 18 all mask files are in the I band\n",
    "        filenames_dict[f][\"mask\"] = [\n",
    "            os.path.join(data_path, f\"{tract_patch}_scarlet_segmask.fits\") for tract_patch in s\n",
    "        ]\n",
    "\n",
    "    filenames_dict_list.append(filenames_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3c81a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11889 2364 2388\n",
      "0.19883926318445622\n"
     ]
    }
   ],
   "source": [
    "ftrain = glob.glob(os.path.join(\"/home/shared/hsc/HSC/HSC_DR3/data/train/\", \"*_scarlet_segmask.fits\"))\n",
    "ftest = glob.glob(os.path.join(\"/home/shared/hsc/HSC/HSC_DR3/data/test/\", \"*_scarlet_segmask.fits\"))\n",
    "fval = glob.glob(os.path.join(\"/home/shared/hsc/HSC/HSC_DR3/data/val/\", \"*_scarlet_segmask.fits\"))\n",
    "print(len(ftrain), len(ftest), len(fval))\n",
    "\n",
    "print(len(ftest) / len(ftrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ccd77e9-d664-4dc3-8c43-389fe017befc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train sample:  2000\n",
      "# of test sample:  2000\n"
     ]
    }
   ],
   "source": [
    "# number of total samples\n",
    "print(\"# of train sample: \", len(filenames_dict_list[0][\"g\"][\"img\"]))\n",
    "print(\"# of test sample: \", len(filenames_dict_list[1][\"g\"][\"img\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebf615",
   "metadata": {},
   "source": [
    "### We then format the data into the dictionary format required by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df763913-3b26-465a-bddd-c8088b3d065a",
   "metadata": {},
   "outputs": [],
   "source": [
    "garbage_data = []\n",
    "\n",
    "\n",
    "def get_astro_dicts(filename_dict):\n",
    "\n",
    "    \"\"\"\n",
    "    This can needs to be customized to your trianing data format\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_dicts = []\n",
    "    filters = list(filename_dict.keys())\n",
    "    # yufeng april5: why only 1st filter\n",
    "    f = filename_dict[\"filters\"][0]  # Pick the 1st filter for now\n",
    "\n",
    "    # Filename loop\n",
    "    for idx, (filename_img, filename_mask) in enumerate(\n",
    "        zip(filename_dict[f][\"img\"], filename_dict[f][\"mask\"])\n",
    "    ):\n",
    "        record = {}\n",
    "\n",
    "        # Open FITS image of first filter (each should have same shape)\n",
    "        with fits.open(filename_img, memmap=False, lazy_load_hdus=False) as hdul:\n",
    "            height, width = hdul[0].data.shape\n",
    "\n",
    "        # Open each FITS mask image\n",
    "        with fits.open(filename_mask, memmap=False, lazy_load_hdus=False) as hdul:\n",
    "            hdul = hdul[1:]\n",
    "            sources = len(hdul)\n",
    "            # Normalize data\n",
    "            data = [hdu.data for hdu in hdul]\n",
    "            category_ids = [hdu.header[\"CAT_ID\"] for hdu in hdul]\n",
    "            ellipse_pars = [hdu.header[\"ELL_PARM\"] for hdu in hdul]\n",
    "            bbox = [list(map(int, hdu.header[\"BBOX\"].split(\",\"))) for hdu in hdul]\n",
    "            area = [hdu.header[\"AREA\"] for hdu in hdul]\n",
    "\n",
    "        # Add image metadata to record (should be the same for each filter)\n",
    "        for f in filename_dict[\"filters\"]:\n",
    "            record[f\"filename_{f.upper()}\"] = filename_dict[f][\"img\"][idx]\n",
    "        # Assign file_name\n",
    "        record[f\"file_name\"] = filename_dict[filename_dict[\"filters\"][0]][\"img\"][idx]\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "        objs = []\n",
    "\n",
    "        # Generate segmentation masks from model\n",
    "        for i in range(sources):\n",
    "            image = data[i]\n",
    "            # Why do we need this?\n",
    "            if len(image.shape) != 2:\n",
    "                continue\n",
    "            height_mask, width_mask = image.shape\n",
    "            # Create mask from threshold\n",
    "            mask = data[i]\n",
    "            # Smooth mask\n",
    "            # mask = cv2.GaussianBlur(mask, (9,9), 2)\n",
    "            x, y, w, h = bbox[i]  # (x0, y0, w, h)\n",
    "\n",
    "            # https://github.com/facebookresearch/Detectron/issues/100\n",
    "            contours, hierarchy = cv2.findContours(\n",
    "                (mask).astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n",
    "            )\n",
    "            segmentation = []\n",
    "            for contour in contours:\n",
    "                # contour = [x1, y1, ..., xn, yn]\n",
    "                contour = contour.flatten()\n",
    "                if len(contour) > 4:\n",
    "                    contour[::2] += x - w // 2\n",
    "                    contour[1::2] += y - h // 2\n",
    "                    segmentation.append(contour.tolist())\n",
    "            # No valid countors\n",
    "            if len(segmentation) == 0:\n",
    "                continue\n",
    "\n",
    "            # Add to dict\n",
    "            obj = {\n",
    "                \"bbox\": [x - w // 2, y - h // 2, w, h],\n",
    "                \"area\": w * h,\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": segmentation,\n",
    "                \"category_id\": category_ids[i],\n",
    "                \"ellipse_pars\": ellipse_pars[i],\n",
    "            }\n",
    "            objs.append(obj)\n",
    "\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "        # img = read_image(record, normalize=\"lupton\", stretch=.8, Q=150, ceil_percentile=99.9995)\n",
    "        # img0std = np.std(np.trim_zeros(img[:,:,0].flatten()))\n",
    "        # if img0std > 10:\n",
    "        #    img1std = np.std(np.trim_zeros(img[:,:,1].flatten()))\n",
    "        #    if img1std > 10:\n",
    "        #        img2std = np.std(np.trim_zeros(img[:,:,2].flatten()))\n",
    "        #        if img2std > 10:\n",
    "        #            dataset_dicts.append(record)\n",
    "        #        else:\n",
    "        #            garbage_data.append(record)\n",
    "        #    else:\n",
    "        #        garbage_data.append(record)\n",
    "        # else:\n",
    "        #    garbage_data.append(record)\n",
    "\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6d628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_json(dict_list, name, output_file, allow_cached=True):\n",
    "    \"\"\"\n",
    "    Converts dataset into COCO format and saves it to a json file.\n",
    "    dataset_name must be registered in DatasetCatalog and in detectron2's standard format.\n",
    "\n",
    "    Args:\n",
    "        dataset_name:\n",
    "            reference from the config file to the catalogs\n",
    "            must be registered in DatasetCatalog and in detectron2's standard format\n",
    "        output_file: path of json file that will be saved to\n",
    "        allow_cached: if json file is already present then skip conversion\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: The dataset or the conversion script *may* change,\n",
    "    # a checksum would be useful for validating the cached data\n",
    "\n",
    "    PathManager.mkdirs(os.path.dirname(output_file))\n",
    "    with file_lock(output_file):\n",
    "        if PathManager.exists(output_file) and allow_cached:\n",
    "            logger.warning(\n",
    "                f\"Using previously cached COCO format annotations at '{output_file}'. \"\n",
    "                \"You need to clear the cache file if your dataset has been modified.\"\n",
    "            )\n",
    "        else:\n",
    "\n",
    "            print(f\"Caching COCO format annotations at '{output_file}' ...\")\n",
    "            tmp_file = output_file + \".tmp\"\n",
    "            with PathManager.open(tmp_file, \"w\") as f:\n",
    "                json.dump(dict_list, f)\n",
    "            shutil.move(tmp_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd7af981-dbe5-47cd-8942-a785a90f841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Loading test\n",
      "Loading val\n",
      "Took  3311.316450357437 seconds for  2000  samples\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "dataset_dicts = {}\n",
    "for i, d in enumerate(dataset_names):\n",
    "    print(f\"Loading {d}\")\n",
    "    dataset_dicts[d] = get_astro_dicts(filenames_dict_list[i])\n",
    "\n",
    "print(\"Took \", time.time() - t0, \"seconds for \", sampleNumbers, \" samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95494db",
   "metadata": {},
   "source": [
    "### Now save the dictionaries to the disc.  Once they are saved, you can read them into memory in very little time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a54e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching COCO format annotations at '/home/shared/hsc/HSC/HSC_DR3/data/train.json' ...\n"
     ]
    }
   ],
   "source": [
    "# convert_to_coco_json('astro_train', os.path.join(dirpath,'train.json'), allow_cached=True)\n",
    "\n",
    "convert_to_json(\n",
    "    dataset_dicts[\"train\"], \"astro_train\", os.path.join(dirpath, \"train.json\"), allow_cached=False\n",
    ")\n",
    "convert_to_json(dataset_dicts[\"test\"], \"astro_test\", os.path.join(dirpath, \"test.json\"), allow_cached=False)\n",
    "convert_to_json(dataset_dicts[\"val\"], \"astro_val\", os.path.join(dirpath, \"val.json\"), allow_cached=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3072556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_json(file):\n",
    "    # Opening JSON file\n",
    "    with open(file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6a51b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train\n",
      "Loading test\n",
      "Loading val\n",
      "Took  1.2500059604644775 seconds to load \n"
     ]
    }
   ],
   "source": [
    "datadir = \"/home/shared/hsc/HSC/HSC_DR3/data/\"\n",
    "t0 = time.time()\n",
    "dataset_dicts = {}\n",
    "for i, d in enumerate(dataset_names):\n",
    "    print(f\"Loading {d}\")\n",
    "    dataset_dicts[d] = get_data_from_json(datadir + dataset_names[i] + \".json\")\n",
    "\n",
    "print(\"Took \", time.time() - t0, \"seconds to load \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e605d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-astrodetnv]",
   "language": "python",
   "name": "conda-env-.conda-astrodetnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
